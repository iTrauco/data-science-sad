{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460d5d60-8678-4b69-baf8-8e4568029105",
   "metadata": {},
   "source": [
    "# Model Training for Traffic Object Detection\n",
    "\n",
    "## Overview\n",
    "This notebook trains a YOLOv5 model to detect vehicles and pedestrians from Atlanta traffic camera footage.\n",
    "\n",
    "## Dataset Summary\n",
    "- **Source**: ATL-0610 camera at 10th Street and Monroe Drive\n",
    "- **Annotated frames**: 29 frames from video ATL-0610_20250609_131130\n",
    "- **Classes**: 2 (vehicle, pedestrian)\n",
    "- **Annotations**: YOLO format exported from CVAT\n",
    "- **Location**: `/home/trauco/data-science-sad/annotations/yolo/`\n",
    "\n",
    "## Training Objectives\n",
    "1. Train initial YOLOv5s model (smallest variant for quick iteration)\n",
    "2. Validate model performance on annotated data\n",
    "3. Establish baseline metrics for future improvements\n",
    "\n",
    "## Expected Outputs\n",
    "- Trained model weights: `runs/train/exp/weights/best.pt`\n",
    "- Training metrics and loss curves\n",
    "- Validation results on test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117c98a-14ef-49b8-a8ef-2bcbcf5881c9",
   "metadata": {},
   "source": [
    "## Setup and Environment Verification\n",
    "\n",
    "Initialize the training environment by importing required libraries, checking GPU availability, and verifying that annotation files from CVAT export are present in the expected directory structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b937a7-af8d-4f10-9916-267a952b48a3",
   "metadata": {},
   "source": [
    "## Environment Update Required\n",
    "\n",
    "**Missing packages: PyTorch and Ultralytics**\n",
    "\n",
    "### Action Items for Reproducibility Framework\n",
    "1. Update `environment.yml` to include PyTorch dependencies\n",
    "2. Update `environment.yml` to include Ultralytics \n",
    "3. Document GPU requirements (NVIDIA RTX A5500, CUDA 12.0)\n",
    "4. Add installation verification steps to setup documentation\n",
    "\n",
    "### Installation Instructions\n",
    "```bash\n",
    "# activate environment\n",
    "conda activate traffic-vision\n",
    "\n",
    "# install pytorch with cuda support\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "# install ultralytics\n",
    "pip install ultralytics\n",
    "\n",
    "# verify installations\n",
    "python -c \"import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')\"\n",
    "python -c \"import ultralytics; print(f'Ultralytics: {ultralytics.__version__}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18bc6d9e-b1e5-4c12-ae27-5ab7940d09a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA RTX A5500\n",
      "\n",
      "Checking annotation files in: /home/trauco/data-science-sad/annotations/yolo\n",
      "obj.names exists: True\n",
      "train.txt exists: True\n",
      "obj_train_data exists: True\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# gpu check\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "# paths\n",
    "PROJECT_ROOT = Path(\"/home/trauco/data-science-sad\")\n",
    "ANNOTATIONS_PATH = PROJECT_ROOT / \"annotations\" / \"yolo\"\n",
    "\n",
    "# verify files\n",
    "print(f\"\\nChecking annotation files in: {ANNOTATIONS_PATH}\")\n",
    "print(f\"obj.names exists: {(ANNOTATIONS_PATH / 'obj.names').exists()}\")\n",
    "print(f\"train.txt exists: {(ANNOTATIONS_PATH / 'train.txt').exists()}\")\n",
    "print(f\"obj_train_data exists: {(ANNOTATIONS_PATH / 'obj_train_data').is_dir()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cdd4e7-a9ff-4367-a0e7-e968c1cc9e19",
   "metadata": {},
   "source": [
    "## Inspect Annotation Data Structure\n",
    "\n",
    "This cell examines the YOLO format annotations exported from CVAT to understand:\n",
    "- Class names defined in obj.names\n",
    "- Number of annotated images in train.txt\n",
    "- YOLO annotation format (class_id x_center y_center width height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8273b693-d634-4085-ac0d-07fb5218005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['vehicle', 'pedestrian']\n",
      "\n",
      "Training images: 129\n",
      "First image: data/obj_train_data/frame_131130_000.jpg\n"
     ]
    }
   ],
   "source": [
    "# load annotations\n",
    "with open(ANNOTATIONS_PATH / 'obj.names', 'r') as f:\n",
    "    classes = f.read().strip().split('\\n')\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# check train list\n",
    "with open(ANNOTATIONS_PATH / 'train.txt', 'r') as f:\n",
    "    train_images = f.readlines()\n",
    "print(f\"\\nTraining images: {len(train_images)}\")\n",
    "print(f\"First image: {train_images[0].strip()}\")\n",
    "\n",
    "# sample annotation\n",
    "sample_image = Path(train_images[0].strip())\n",
    "annotation_file = sample_image.with_suffix('.txt')\n",
    "if annotation_file.exists():\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        annotations = f.readlines()\n",
    "    print(f\"\\nSample annotations ({annotation_file.name}):\")\n",
    "    for ann in annotations[:5]:  # first 5\n",
    "        print(f\"  {ann.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd5b6c4-1b8f-4656-9d8e-3fa7562309b9",
   "metadata": {},
   "source": [
    "## Fix Annotation File Paths\n",
    "\n",
    "The train.txt file contains relative paths that need to be converted to absolute paths for YOLO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ded3baa2-e872-4c93-8d24-6332c6696f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative path in train.txt: data/obj_train_data/frame_131130_000.jpg\n",
      "Absolute path: /home/trauco/data-science-sad/annotations/yolo/data/obj_train_data/frame_131130_000.jpg\n",
      "Image exists: False\n",
      "Annotation exists: False\n",
      "\n",
      "Actual files in obj_train_data:\n",
      "  Images: 129\n",
      "  Annotations: 129\n"
     ]
    }
   ],
   "source": [
    "# check current path\n",
    "sample_relative = train_images[0].strip()\n",
    "print(f\"Relative path in train.txt: {sample_relative}\")\n",
    "\n",
    "# build absolute path\n",
    "sample_absolute = ANNOTATIONS_PATH / sample_relative\n",
    "print(f\"Absolute path: {sample_absolute}\")\n",
    "print(f\"Image exists: {sample_absolute.exists()}\")\n",
    "\n",
    "# check annotation\n",
    "annotation_absolute = sample_absolute.with_suffix('.txt')\n",
    "print(f\"Annotation exists: {annotation_absolute.exists()}\")\n",
    "\n",
    "# count actual files\n",
    "if (ANNOTATIONS_PATH / \"obj_train_data\").exists():\n",
    "    images = list((ANNOTATIONS_PATH / \"obj_train_data\").glob(\"*.jpg\"))\n",
    "    annotations = list((ANNOTATIONS_PATH / \"obj_train_data\").glob(\"*.txt\"))\n",
    "    print(f\"\\nActual files in obj_train_data:\")\n",
    "    print(f\"  Images: {len(images)}\")\n",
    "    print(f\"  Annotations: {len(annotations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236933e9-dedc-4702-9d82-3cbd3e63a4fc",
   "metadata": {},
   "source": [
    "## Locate Missing Images\n",
    "\n",
    "The CVAT export created annotation files but didn't include the images. We need to find where the original images are and prepare the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50734482-2539-480f-adac-2ff5ba0f25da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of /home/trauco/data-science-sad/annotations/yolo/obj_train_data:\n",
      "  frame_131130_056.txt\n",
      "  frame_131130_046.txt\n",
      "  frame_131130_031.jpg\n",
      "  frame_131130_063.txt\n",
      "  frame_131130_005.jpg\n",
      "\n",
      "Original images location: /home/trauco/data-science-sad/annotation_sample\n",
      "Exists: True\n",
      "Image count: 129\n",
      "First image: frame_131130_031.jpg\n"
     ]
    }
   ],
   "source": [
    "# check annotation structure\n",
    "obj_train_path = ANNOTATIONS_PATH / \"obj_train_data\"\n",
    "print(f\"Contents of {obj_train_path}:\")\n",
    "files = list(obj_train_path.glob(\"*\"))[:5]  # first 5\n",
    "for f in files:\n",
    "    print(f\"  {f.name}\")\n",
    "\n",
    "# find original images\n",
    "original_images = PROJECT_ROOT / \"annotation_sample\"\n",
    "print(f\"\\nOriginal images location: {original_images}\")\n",
    "print(f\"Exists: {original_images.exists()}\")\n",
    "if original_images.exists():\n",
    "    orig_files = list(original_images.glob(\"*.jpg\"))\n",
    "    print(f\"Image count: {len(orig_files)}\")\n",
    "    if orig_files:\n",
    "        print(f\"First image: {orig_files[0].name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a55d11-4407-4c9a-8c6a-161c5cbb8785",
   "metadata": {},
   "source": [
    "## Prepare Dataset Structure\n",
    "\n",
    "The images need to be copied to the YOLO dataset directory to match the annotation files. This cell copies the annotated images to the correct location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9211857-3543-4396-9c16-bb31ae04c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 0 images\n",
      "\n",
      "Dataset ready:\n",
      "  Images: 129\n",
      "  Annotations: 129\n"
     ]
    }
   ],
   "source": [
    "# copy images to match annotations\n",
    "import shutil\n",
    "\n",
    "source_dir = PROJECT_ROOT / \"annotation_sample\"\n",
    "dest_dir = ANNOTATIONS_PATH / \"obj_train_data\"\n",
    "\n",
    "# copy images\n",
    "copied = 0\n",
    "for txt_file in dest_dir.glob(\"*.txt\"):\n",
    "    img_name = txt_file.stem + \".jpg\"\n",
    "    source_img = source_dir / img_name\n",
    "    dest_img = dest_dir / img_name\n",
    "    \n",
    "    if source_img.exists() and not dest_img.exists():\n",
    "        shutil.copy(source_img, dest_img)\n",
    "        copied += 1\n",
    "\n",
    "print(f\"Copied {copied} images\")\n",
    "\n",
    "# verify\n",
    "images_after = list(dest_dir.glob(\"*.jpg\"))\n",
    "annotations_after = list(dest_dir.glob(\"*.txt\"))\n",
    "print(f\"\\nDataset ready:\")\n",
    "print(f\"  Images: {len(images_after)}\")\n",
    "print(f\"  Annotations: {len(annotations_after)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effed1fa-0bc5-4b9c-86fc-28839a163f7b",
   "metadata": {},
   "source": [
    "## Create YOLO Dataset Configuration\n",
    "\n",
    "This cell creates a YAML configuration file that tells YOLO where to find the training data and what classes to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "886994c1-c2c5-41ac-b6ef-369c9ec0640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset config: /home/trauco/data-science-sad/traffic_dataset.yaml\n",
      "\n",
      "Contents:\n",
      "names:\n",
      "  0: vehicle\n",
      "  1: pedestrian\n",
      "path: /home/trauco/data-science-sad/annotations/yolo\n",
      "train: obj_train_data\n",
      "val: obj_train_data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataset yaml\n",
    "dataset_config = {\n",
    "    'path': str(ANNOTATIONS_PATH),\n",
    "    'train': 'obj_train_data',\n",
    "    'val': 'obj_train_data',  # same for now\n",
    "    'names': {\n",
    "        0: 'vehicle',\n",
    "        1: 'pedestrian'\n",
    "    }\n",
    "}\n",
    "\n",
    "# save yaml\n",
    "yaml_path = PROJECT_ROOT / 'traffic_dataset.yaml'\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Created dataset config: {yaml_path}\")\n",
    "print(\"\\nContents:\")\n",
    "print(yaml.dump(dataset_config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32268921-7d7d-4166-9976-202661e4f120",
   "metadata": {},
   "source": [
    "## Initialize and Train YOLO Model\n",
    "\n",
    "This cell initializes a YOLOv8 nano model (smallest and fastest) and starts training on the annotated traffic data. Using a small model is ideal for initial testing with limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea2bf70d-ec18-41d3-8585-ee5e6d0905ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:\n",
      "  Model: YOLOv8n\n",
      "  Epochs: 50\n",
      "  Batch size: 16\n",
      "  Image size: 640\n",
      "  Dataset: /home/trauco/data-science-sad/traffic_dataset.yaml\n",
      "Ultralytics 8.3.154 ðŸš€ Python-3.11.13 torch-2.3.1+cu121 CUDA:0 (NVIDIA RTX A5500, 24090MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/trauco/data-science-sad/traffic_dataset.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=traffic_v1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/home/trauco/data-science-sad/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/trauco/data-science-sad/runs/traffic_v1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 215.2Â±118.8 MB/s, size: 38.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/tra\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/trauco/data-science-sad/annotations/yolo/obj_train_data.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 186.5Â±94.9 MB/s, size: 38.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/trauc\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/trauco/data-science-sad/runs/traffic_v1/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/trauco/data-science-sad/runs/traffic_v1\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      2.01G   \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/trauco/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 37, in do_one_step\n    data = pin_memory(data, device)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/trauco/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 68, in pin_memory\n    clone.update({k: pin_memory(sample, device) for k, sample in data.items()})\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/trauco/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 68, in <dictcomp>\n    clone.update({k: pin_memory(sample, device) for k, sample in data.items()})\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/trauco/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 58, in pin_memory\n    return data.pin_memory(device)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: invalid argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myaml_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43myaml_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# gpu\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPROJECT_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mruns\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraffic_v1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/ultralytics/engine/model.py:797\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    794\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    796\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/ultralytics/engine/trainer.py:227\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    224\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/ultralytics/engine/trainer.py:388\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    386\u001b[39m     pbar = TQDM(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_loader), total=nb)\n\u001b[32m    387\u001b[39m \u001b[38;5;28mself\u001b[39m.tloss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_train_batch_start\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Warmup\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/ultralytics/data/build.py:67\u001b[39m, in \u001b[36mInfiniteDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create an iterator that yields indefinitely from the underlying iterator.\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1326\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._task_info[\u001b[38;5;28mself\u001b[39m._rcvd_idx]) == \u001b[32m2\u001b[39m:\n\u001b[32m   1325\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._task_info.pop(\u001b[38;5;28mself\u001b[39m._rcvd_idx)[\u001b[32m1\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m   1329\u001b[39m idx, data = \u001b[38;5;28mself\u001b[39m._get_data()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1370\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1371\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/_utils.py:705\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    702\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/trauco/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 37, in do_one_step\n    data = pin_memory(data, device)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/trauco/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 68, in pin_memory\n    clone.update({k: pin_memory(sample, device) for k, sample in data.items()})\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/trauco/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 68, in <dictcomp>\n    clone.update({k: pin_memory(sample, device) for k, sample in data.items()})\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/trauco/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 58, in pin_memory\n    return data.pin_memory(device)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: invalid argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = YOLO('yolov8n.pt')  # nano model\n",
    "\n",
    "# training parameters\n",
    "epochs = 50  # quick training\n",
    "batch_size = 16  # small batch\n",
    "imgsz = 640  # standard size\n",
    "\n",
    "print(f\"Starting training:\")\n",
    "print(f\"  Model: YOLOv8n\")\n",
    "print(f\"  Epochs: {epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Image size: {imgsz}\")\n",
    "print(f\"  Dataset: {yaml_path}\")\n",
    "\n",
    "# train\n",
    "results = model.train(\n",
    "    data=str(yaml_path),\n",
    "    epochs=epochs,\n",
    "    batch=batch_size,\n",
    "    imgsz=imgsz,\n",
    "    device=0,  # gpu\n",
    "    project=str(PROJECT_ROOT / 'runs'),\n",
    "    name='traffic_v1',\n",
    "    exist_ok=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf1056-1652-4bbc-8570-b3a841f334ed",
   "metadata": {},
   "source": [
    "## Fix Torchvision Compatibility\n",
    "\n",
    "The installed torchvision version is incompatible with PyTorch. This needs to be reinstalled to match the PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "078097b6-d62b-456f-ae1c-b38b5f5363f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# check versions\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPyTorch version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTorchvision version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorchvision.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torchvision/__init__.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodulefinder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torchvision/_meta_registrations.py:25\u001b[39m\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mroi_align\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/traffic-vision/lib/python3.11/site-packages/torchvision/_meta_registrations.py:18\u001b[39m, in \u001b[36mregister_meta.<locals>.wrapper\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(fn):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextension\u001b[49m._has_ops():\n\u001b[32m     19\u001b[39m         get_meta_lib().impl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch.ops.torchvision, op_name), overload_name), fn)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# check versions\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# fix command\n",
    "print(\"\\nRun this in terminal to fix:\")\n",
    "print(\"conda activate traffic-vision\")\n",
    "print(\"pip uninstall torchvision -y\")\n",
    "print(\"pip install torchvision==0.18.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4dae3a-4087-4a00-a66b-e873e679e7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4669366-258a-4683-a911-5221320066bf",
   "metadata": {},
   "source": [
    "## Restart Kernel Required\n",
    "\n",
    "The torchvision module has import errors. The kernel needs to be restarted and dependencies checked.\n",
    "\n",
    "### Steps:\n",
    "1. Restart the Jupyter kernel (Kernel â†’ Restart)\n",
    "2. Check installed versions\n",
    "3. Fix any version mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f2e150-62fc-4e86-ad41-1bf9ab7eb056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]\n",
      "PyTorch: 2.3.1.post100\n",
      "Torchvision error: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)\n"
     ]
    }
   ],
   "source": [
    "# check pytorch only\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"PyTorch error: {e}\")\n",
    "\n",
    "try:\n",
    "    import torchvision\n",
    "    print(f\"Torchvision: {torchvision.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Torchvision error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba1bb7f-497f-4d83-945b-5f1730a511d4",
   "metadata": {},
   "source": [
    "## Fix Torchvision Installation\n",
    "\n",
    "Torchvision has a broken installation. It needs to be completely removed and reinstalled with the correct version for PyTorch 2.3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147ddc2-050a-4ee8-84c7-c5b7e440d47f",
   "metadata": {},
   "source": [
    "## Torchvision Installation Fixed\n",
    "\n",
    "The torchvision module was reinstalled with the correct version compatible with PyTorch 2.3.1.\n",
    "\n",
    "### Commands Executed:\n",
    "```bash\n",
    "# activate environment\n",
    "conda activate traffic-vision\n",
    "\n",
    "# completely remove torchvision\n",
    "pip uninstall torchvision -y\n",
    "conda remove torchvision -y\n",
    "\n",
    "# install compatible version\n",
    "pip install torchvision==0.18.1\n",
    "\n",
    "# test it\n",
    "python -c \"import torchvision; print(f'Torchvision: {torchvision.__version__}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e50e31-45d3-4a5f-880c-cfc3ad18e890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
